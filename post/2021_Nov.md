# Oct, 2021

## 2021-10-28: Megalodon rf - Optimization of 2021-10-24

Script `2021-10-28/external_test.py` for The script is used for external testing, an optimized version of `2021-10-24/run_model.py`

Load the model -> load the data -> save the test score/class report/confusion matrix/confusion matrix visualization on testing dataset

Usage: `python external_test.py --input_path $input_path --model_path $model_path --output_path $output_path`

## 2021-10-26: Megalodon rf - Use the best parameter, but change the downsamping category with RandomUnderSampler

GridSearch for the best unsampling strategy with best parameter from 2021-10-08

Best Parameters:  {'u__sampling_strategy': {0: 319104, 1: 496384, 2: 1108}}  

Best f1_macro in cv: 0.6286333169961849 

f1_macro test score: 0.6290468261812278

```python
 #####Prepare downsamping_strategy
    value_sub1 = [(9*i*1108, 14*i*1108, 1108) for i in range(2,40,2)]
    value_sub2 = [(36*i*1108, 56*i*1108, 1108) for i in range(10,110,5)]
    value = list()
    value = value_sub1 + value_sub2
    key=[0,1,2]
    for i in value:
        item = dict(zip(key, i)).copy()
        sampling_strategy.append(item)

    params = {
        'u__sampling_strategy' : sampling_strategy} 
```

## 2021-10-24: Megalodon rf - Rerun the rf model with best parameter from 2021-10-08

Use the parameter setting from 2021-10-08 to rerun the model instead of loading the saved pkl model `2021-10-24/run_model.py`

Result: The result is similar to `2021-10-23/load_pkl.sh`, but not exactly the same

Mean accuracy on cv: 0.894

Mean recall_macro on cv: 0.881

Mean f1_macro on cv: 0.628

Accuracy on testing dataset: 0.893609903946166

Confusion matrix(per-class): 5C: 88.6%(1306108/1473652), 89.8%(2088837/2325469), 86.3%(358/415)

```python
    #Split the data into training and testing
    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                        test_size=0.2,
                                                        stratify=y,
                                                        random_state=42)
    print("Spliting is done!")
    print("Before the pipeline:\n y_train:{},\n y_test: {}".format(Counter(y_train), Counter(y_test)))

    # Define cross-validation fold
    stratified_kfold = StratifiedKFold(n_splits = 3, random_state = 42, shuffle = True)
    
    model_best = imbpipeline(steps=[('o', SMOTE(random_state=42)),
                                    ('m', RandomForestClassifier(max_depth=5, min_samples_leaf=2,n_estimators=80, random_state=42))]) 
```


## 2021-10-23: Megalodon rf

Best parameter set from 2021-10-08, result is shown in `result/result.md`:
```shell
Best parameter: 
The context in the () is the default setting
(k_neighbors=5), n_estimators=80, (max_features = 'auto'), max_depth=5, (min_samples_split=2), min_samples_leaf=2, (bootstrap=True), random_state=42
```

1. Load the saved pkl model:  `2021-10-23/load_pkl.py`, `2021-10-23/load_pkl.sh`

Usage: `python load_pkl.py --input_path $input_path --pkl_path $pkl_path --output_path $output_path`

Workflow: Load the model with best parameter set -> load the data -> get the cv result on training dataset -> get the  test score/class report/confusion matrix on testing dataset

It is an optimized version of `2021-10-08/read_pkl.py`

2. Do hyperparameter tuning around the best parameter sets for 2021-10-08 with GridSearch with 48 parameter sets(`rf_SMOTE_GridSearch.py`), randomSearch with 20 parameter sets(`rf_SMOTE_randomCV_20.py`) or randomSearch with 120 parameter sets(`rf_SMOTE_randomCV.py`)


3. Modify the parameters from `2021-10-21/p2_rf_ENN_SMOTE_RandomCV.py` to rerun the ENN -> SMOTE -> random forest(Random search)

```python
    ###Build the pipeline
    rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
    under = EditedNearestNeighbours(n_jobs = -1)
    over = SMOTE(sampling_strategy='not majority', random_state = 42, n_jobs = -1)
    steps = [('u', under), ('o', over), ('m', rf_model)]

    pipeline = imbpipeline(steps=steps)       

    # Define cross-validation fold
    stratified_kfold = StratifiedKFold(n_splits = 3, random_state = 42, shuffle = True)
    
    #https://stackoverflow.com/questions/51480776/how-to-implement-ratio-based-smote-oversampling-while-cv-ing-dataset
    ##parameter testing
    #assign the parameters to the named step in the pipeline
    params = {
        'u__n_neighbors' : [3, 5, 10, 20],
        'o__k_neighbors':[5, 10, 20],      
        'm__n_estimators': [i for i in range(10, 50, 5)], # number of trees in the random forest
              'm__max_features': ['auto'], # number of features in consideration at every split
              'm__max_depth': [i for i in range(5,30,5)], # maximum number of levels allowed in each decision tree
              'm__min_samples_split': [2,5,10], # minimum sample number to split a node
              'm__min_samples_leaf': [1,2], # minimum sample number that can be stored in a leaf node
              'm__bootstrap': [True, False]} # method used to sample data points"

    # RandomSearch
    pipe_search = RandomizedSearchCV(estimator = pipeline, 
                                     param_distributions = params, 
                                     scoring='f1_macro', 
                                     cv=stratified_kfold, 
                                     verbose=3,   
                                     n_iter=120,
                                     return_train_score=True, 
                                     n_jobs=-1)
```

## 2021-10-22: Megalodon rf

Write a script to explore differnt n_neighbor in ENN for `2021-10-21/p2_rf_ENN_SMOTE_RandomCV.py`

EditedNearestNeighbours default n_neighbors = 3

https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.EditedNearestNeighbours.html

## 2021-10-21: Megalodon rf - Downsampling(RandomUnderSampler/ENN) -> oversampling(SMOTE) -> random forest

Build 2 pipelines:

1. RandomUnderSampler -> SMOTE -> random forest(Random search): `p1_rf_RandomUnderSampler_SMOTE_RandomCV.py`

RandomUnderSampler requires the sampling_strategy with exact number for each category
```python
    # Pipeline strategy: RandomUnderSampler -> SMOTE -> rf
    ###Build the pipeline
    rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
    under = RandomUnderSampler(random_state = 42)
    over = SMOTE(sampling_strategy='not majority', random_state = 42, n_jobs = -1)
    steps = [('u', under), ('o', over), ('m', rf_model)]
    pipeline = imbpipeline(steps=steps)
    #https://stackoverflow.com/questions/51480776/how-to-implement-ratio-based-smote-oversampling-while-cv-ing-dataset
    ##parameter testing
    #assign the parameters to the named step in the pipeline
    params = {
        'u__sampling_strategy' : [{0: 39888, 1:62048, 2:1108}, {0: 9972, 1:15512, 2:1108}, 
                                  {0: 19944, 1:31024, 2:1108}, {0: 398880, 1:620480, 2:1108}, 
                                  {0: 199440, 1:310240, 2:1108}],
        'o__k_neighbors':[5, 10, 50, 100],
        'm__n_estimators': [i for i in range(60, 150, 20)], # number of trees in the random forest
        'm__max_features': ['auto'], # number of features in consideration at every split
        'm__max_depth': [i for i in range(5,30,5)], # maximum number of levels allowed in each decision tree
        'm__min_samples_split': [2,5,10], # minimum sample number to split a node
        'm__min_samples_leaf': [1,2], # minimum sample number that can be stored in a leaf node
        'm__bootstrap': [True, False] # method used to sample data points"
             } 
    # Define cross-validation fold
    stratified_kfold = StratifiedKFold(n_splits = 3, random_state = 42, shuffle = True)
    # RandomSearch
    pipe_search = RandomizedSearchCV(estimator = pipeline, 
                                     param_distributions = params, 
                                     scoring='f1_macro', 
                                     cv=stratified_kfold, 
                                     verbose=3,   
                                     n_iter=120,
                                     return_train_score=True, 
                                     n_jobs=-1)
```

2. EditedNearestNeighbours(ENN) -> SMOTE -> random forest(Random search): `p2_rf_ENN_SMOTE_RandomCV.py`

```python
    # Pipeline strategy: RandomUnderSampler -> SMOTE -> rf
    ###Build the pipeline
    rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
    under = EditedNearestNeighbours(n_jobs = -1)
    over = SMOTE(sampling_strategy='not majority', random_state = 42, n_jobs = -1)
    steps = [('u', under), ('o', over), ('m', rf_model)]

    pipeline = imbpipeline(steps=steps)

    #https://stackoverflow.com/questions/51480776/how-to-implement-ratio-based-smote-oversampling-while-cv-ing-dataset
    ##parameter testing
    #assign the parameters to the named step in the pipeline
    params = {
        'u__n_neighbors' : [20, 50, 100],
        'o__k_neighbors':[5, 10, 50, 100],      
        'm__n_estimators': [i for i in range(10, 50, 5)], # number of trees in the random forest
              'm__max_features': ['auto'], # number of features in consideration at every split
              'm__max_depth': [i for i in range(5,30,5)], # maximum number of levels allowed in each decision tree
              'm__min_samples_split': [2,5,10], # minimum sample number to split a node
              'm__min_samples_leaf': [1,2], # minimum sample number that can be stored in a leaf node
              'm__bootstrap': [True, False]} # method used to sample data points"  

    # Define cross-validation fold
    stratified_kfold = StratifiedKFold(n_splits = 3, random_state = 42, shuffle = True)
    # RandomSearch
    pipe_search = RandomizedSearchCV(estimator = pipeline, 
                                     param_distributions = params, 
                                     scoring='f1_macro', 
                                     cv=stratified_kfold, 
                                     verbose=3,   
                                     n_iter=120,
                                     return_train_score=True, 
                                     n_jobs=-1)
```

## 2021-10-19: Megalodon rf

1. Optimized version for matrix script: `2021-10-19/plot_confusion_matrix.py`

2. Explore the strategy to combine undersampling with Edited Nearest Neighbors and oversampling with SMOTE

## 2021-10-18: model build - Get the feature extraction done

Apply extract feature module for T4_5hmC/lambda_5mC/lambda_5C fast5 files(Guppy v5.0.11+Tombo 1.5.1)

Result: `/pod/2/li-lab/Ziwei/Nanopore/results/feature_Guppy5.0.11/`

Extract features from single fast5 read after basecalling(Guppy v5.0.11) and resquiggle

Signal feature:
1. kmer sequence
2. signal mean for each base in kmer (17-mer)
3. signal std for each base in kmer (17-mer)
4. signal number for each base in kmer (17-mer)
5. signal range for each base in kmer (17-mer)

```shell
#chrom, site_pos, align_strand, loc_in_ref, read_id, read_strand, kmer_seq, kmer_signal_mean, kmer_signal_std, kmer_signal_length, kmer_signal_range,
#mod_label choices=[0,1,2]
(base) [c-panz@sumner-log2 feature_Guppy5.0.11]$ head -3 lambda_5C.csv
J02459.1	42108	-	6393	17ff58dc-512e-4b3a-abb8-5ab1bdb42f6a	t	CGGTGATACTTCGTCGC	-1.099168,-1.107496,-1.508581,0.643744,-1.4208,-1.009653,1.055123,-0.112415,-0.446686,0.724452,0.999244,-0.227034,-1.434034,1.489052,0.192711,-0.661999,0.319133	0.0,0.152239,0.111529,0.187366,0.141536,0.159047,0.5128,0.257095,0.15307,0.250642,0.107862,0.199997,0.205589,0.150345,0.124343,0.115293,0.155229	1,6,9,13,28,6,38,18,21,3,7,17,21,14,14,10,20	0.0,0.424678,0.399697,0.736942,0.549584,0.44966,2.473129,1.13664,0.549585,0.574565,0.362226,0.811886,0.986754,0.599546,0.412188,0.399697,0.587056	0
```

## 2021-10-17: model build

Extract feature module debugging:

Modify the code from 2021-10-12: delete multiprocess.queue() at this moment since it didn't work. Right now it can only used single process without batch size

## 2021-10-14: Other

Test the DeepSignal on its speed and output for its feature extraction module

## 2021-10-13: Megalodon rf

Write a functional optimized script(`rf_pipeline_gridsearch.py`) for SMOTE + GridSearch for random forest model for Megalodon
```shell
script_dir=/pod/2/li-lab/Ziwei/Nanopore/daily/2021-10-13
#input_file=/pod/2/li-lab/Ziwei/Nanopore/daily/test/total.test.bed.gz
input_file='/pod/2/li-lab/Ziwei/Nanopore/results/megalodon2.3.4_guppy5.0.14/total.Megalodon.per_read.prob.bed.gz'
python $script_dir/rf_pipeline_gridsearch.py --input_path $input_file --output_path $script_dir > $script_dir/rf_params_smote_gridsearch.txt
```

## 2021-10-12: model build

Script to extract features from fast5 files: `cytosine/extract_module.py`

Signal feature: 1. kmer sequence 2. signal mean for each base in kmer 3. signal std for each base in kmer 4.signal number for each base in kmer 5. signal range for each base in kmer

## 2021-10-11: Megalodon rf

SMOTE + GridSearch for random forest model for Megalodon

## 2021-10-08: Megalodon rf - Find the best pararmeter set so far
SMOTE + RandomSearch for random forest model for Megalodon

The model with best parameter is saved in `rf.pkl`

I changed the strategy to escape data leakage and follow the steps below(apply SMOTE only on the training dataset, not the whole dataset):
1. Split the sample into training and testing datasets
2. SMOTE to oversample the training dataset
3. Train the the classifier on the training fold for random forest 
4. Validate the classifer on the reamining fold
5. Save the classification report and confusion matrix plotting for default parameter

2. Write a script `read_pkl.py` to read the best model(.pkl)

3. Compare the fast5 files after Guppy-basecalling amd Tombo-requigglling for Guppy v5.0.11/v5.0.14 + Tombo 1.5.1

Conclusion: The fast5 number is smaller in Guppy v5.0.14 than Guppy v5.0.11. T4 is detected with Guppy v5.0.14, I used Guppy v5.0.11 instead.

See result/fast5_count.csv for the exact number

## 2021-10-07: Process

Redo the Guppy-basecalling amd Tombo-requigglling with 1)Guppy v5.0.11 + Tombo 1.5.1  2)Guppy v5.0.14 + Tombo 1.5.1


## 2021-10-05: Process
1. Explore the signal feature after Guppy-basecalling Tombo-requiggled fast5 files: `feature_100521.ipynb`

2. Write `fast5_parser.py` to extract raw signal, event, alignment information from single basecalled-resquirred fast5 read (Exploration: `feature_script.ipynb`)

3. Write `PerChromSeperation.py` to get the list of fast5 files based on chromosome information

